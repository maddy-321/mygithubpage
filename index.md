# Welcome to webpage of Madhur Sabherwal 



## Who am I ?
My name is Madhur Sabherwal 
I am a Data scientist who is Looking to solve problems. currently living in Sydney, Australia. My interests range from Data Science to Machine Learning. i have worked and will be working on Various algorithms and will keep updating my algorithms and work online. 

## Algorithms 

###  Upper Confidence Bound (UCB)

[click here for the repository link ](https://github.com/maddy-321/UCB)

Upper Confidence Bound (UCB) algorithm that overcomes all of the limitations of strategies based on exploration followed by commitment, including the need to know the horizon and sub-optimality gaps. The algorithm has many different forms, depending on the distributional assumptions on the noise.

### Apriori Association Algorithm 

[click here for the repository link ](https://github.com/maddy-321/APRIORI_ASSOCIATION_Algorithm)

Apriori is an algorithm for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database.

### Artificial Neural Network 

[click here for the repository link ](https://github.com/maddy-321/artificial-neural-network)

An artificial neural network is an interconnected group of nodes, similar to the vast network of neurons in a brain.
Here, each circular node represents an artificial neuron and an arrow represents connection from the output of one artificial neuron to the input of another.

###  Decision-Tree-classifiers

[click here for the repository link ](https://github.com/maddy-321/Decision-Tree-classifiers)

The classification technique is a systematic approach to build classification models from an input dat set. For example, decision tree classifiers, rule-based classifiers, neural networks, support vector machines, and naive Bayes classifiers are different technique to solve a classification problem.

### K means 
 
[click here for the repository link ](https://github.com/maddy-321/K_means-)

k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining.

### K nearest Neighbhour 

[click here for the repository link ](https://github.com/maddy-321/K_Nearest_Neighbors)

K nearest neighbor algorithm is very simple. It works based on minimum distance from the query instance to the training samples to determine the K-nearest neighbors. ... The data for KNN algorithm consist of several multivariate attributes name that will be used to classify 

### kernal pca component analysis
[click here for the repository link ](https://github.com/maddy-321/kernel-principal-component-analysis-kernel-PCA-)

In the field of multivariate statistics, kernel principal component analysis (kernel PCA) [1] is an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space.

### Linear-discriminant-analysis-LDA
[click here for the repository link ](https://github.com/maddy-321/Linear-discriminant-analysis-LDA-)

Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more. 

### Logistic_Regression

[click here for the repository link ](https://github.com/maddy-321/Logistic_Regression)

Binary logistic regression is used to predict the odds of being a case based on the values of the independent variables (predictors). The odds are defined as the probability that a particular outcome is a case divided by the probability that it is a non case.


### Multiple_linear_regression

[click here for the repository link ](https://github.com/maddy-321/multiple_linear_regression)

Multiple linear regression (MLR) is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal of multiple linear regression (MLR) is to model the relationship between the explanatory and response variables. 

### Naive BAYES 

[click here for the repository link ](https://github.com/maddy-321/multiple_linear_regression)

In machine learning, naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes has been studied extensively since the 1950s. It was introduced under a different name into the text retrieval community in the early 1960s, and remains a popular (baseline) method for text categorization, the problem of judging documents as belonging to one category or the other (such as spam or legitimate, sports or politics, etc.) with word frequencies as the features. With appropriate pre-processing, it is competitive in this domain with more advanced methods including support vector machines. It also finds application in automatic medical diagnosis.

Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression, which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers. In the statistics and computer science literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.

### Natural_Language_Processing

[click here for the repository link ](https://github.com/maddy-321/Natural_Language_Processing)

Natural language processing is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data.

### principal component analysis 

[click here for the repository link ](https://github.com/maddy-321/Principal-component-analysis)

Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. If there are {\displaystyle n} n observations with {\displaystyle p} p variables, then the number of distinct principal components is {\displaystyle \min(n-1,p)} {\displaystyle \min(n-1,p)}. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.

### Polynomial Regression 

[click here for the repository link ](https://github.com/maddy-321/Polynomial_Regression)

In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x.

### Random Forest Classification.
[click here for the repository link ](https://github.com/maddy-321/Random_Forest_Classification)

Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training setThe first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg. An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered "Random Forests" as a trademark (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's "bagging" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance. 

### Simple Linear Regression 
[click here for the repository link ](https://github.com/maddy-321/Simple_Linear_Regression)

Simple linear regression provides a means to model a straight line relationship between two variables. In classical (or asymmetric ) regression one variable (Y) is called the response or dependent variable, and the other (X) is called the explanatory or independent variable. This is in contrast to correlation where there is no distinction between Y and X in terms of which is an explanatory variable and which a response variable.

 ### Support Vector Machine 
 [click here for the repository link ](https://github.com/maddy-321/Support-Vector-Machine)
 
A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples.

### Thompson Sampling 

 [click here for the repository link ](https://github.com/maddy-321/Thompson-sampling)
 
 In artificial intelligence, Thompson sampling,[1][2] named after William R. Thompson, is a heuristic for choosing actions that addresses the exploration-exploitation dilemma in the multi-armed bandit problem. It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief. 
 
### Xgboost 

 [click here for the repository link ](https://github.com/maddy-321/XGBoost)

XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.

## Projects

### Credit Card Fraud Detection

Throughout the financial sector, machine learning algorithms are being developed to detect fraudulent transactions. In this project, that is exactly what we are going to be doing as well. Using a dataset of of nearly 28,500 credit card transactions and multiple unsupervised anomaly detection algorithms, we are going to identify transactions with a high probability of being credit card fraud. In this project, we will build and deploy the following two machine learning algorithm.
Lo
[click here for the repository link ](https://github.com/maddy-321/credit-card-fraud-detection-/blob/master/Credit%20Card%20Fraud%20Detection%20(Jupyter%20Notebook).ipynb)

### PORTFOLIO FOR COMP 6220 : Data science 

This portfolio is developed as a project in Data science. 
this contain  
Portfolio projects

### Analysis of  data for cycling of professor steve cassidy 
### Analysis of COVID-19 data(Live project)
### Predicting Genre of Book from Summaries

[click here for the repository link ](https://github.com/maddy-321/Portfolio)


### Board Game Prediction 
[click here for the repository link ](https://github.com/maddy-321/BOARD-GAME-REVIEW-PREDICTION---/blob/master/board%20game%20prediction.ipynb)

This work presents a system that facilitates prediction of the winner in a sport game. The system consists of methods for: collection of data from the Internet for games in various sports, preprocessing of the acquired data, feature selection and model building. Many of the algorithms for prediction and classification implemented for this kind of problems.

### Botnet Detection
[click here for the repository link ](https://github.com/maddy-321/BOTNET-DETECTION)

This is work on botnet detection as a comp 6220 : Data Science Project. 

###  Smile Classifier 
[click here for the repository link ](https://github.com/maddy-321/Smile-clasifier-)

The works detects the how smile is classified using Haarcascadde algorithm. 

### FACIAL EXPRESSION RECOGNITION 
[click here for the repository link ](https://github.com/maddy-321/Facial-expression-Recognition-cnn-/blob/main/46160361_Madhur_Sabherwal%20(3).ipynb)

GOT 70 PERCENT ACCURACY USING CNN AND OTHER MODEL IN INCLASS COMPETTION OF MACQUARIE UNIVERSITY

### BITCOIN PRICE PREDICTION USING THE FACEBOOK PROPHET 
[click here for the repository link ](https://github.com/maddy-321/Bitcoin-price-prediction-using-facebook-prophet-)
i HAVE CREATED THIS MODEL TO PREDICT BITCOIN PRICE IN THE COMING NEXT 30 DAYS USING FACEBOOK PROPHET.

## WORK UNDERGOING 
THERE ARE LOTS OF WORK UNDERGOING AND WILL BE UPLOADED SOON. 
